{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Digit Recognizer Kubeflow Pipeline\n",
    "\n",
    "In this [Kaggle competition](https://www.kaggle.com/competitions/digit-recognizer/overview) \n",
    "\n",
    ">MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    ">In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install relevant libraries\n",
    "\n",
    "\n",
    ">Update pip `pip install --user --upgrade pip`\n",
    "\n",
    ">Install and upgrade kubeflow sdk `pip install kfp --upgrade --user --quiet`\n",
    "\n",
    "You may need to restart your notebook kernel after installing the kfp sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "38y13drotnXK",
    "outputId": "61184254-57cb-4f29-c0e5-dba30df0c914",
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (23.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.8 are installed in '/home/chase/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pip-23.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a8V8LN9ttJT"
   },
   "outputs": [],
   "source": [
    "!pip install --user  kfp==1.8.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "taqx2u69tnXS",
    "outputId": "32ec75b1-7d1e-434e-8834-aa841fecd4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp\n",
      "Version: 1.8.11\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: https://github.com/kubeflow/pipelines\n",
      "Author: The Kubeflow Authors\n",
      "Author-email: \n",
      "License: \n",
      "Location: /home/chase/.local/lib/python3.8/site-packages\n",
      "Requires: absl-py, click, cloudpickle, Deprecated, docstring-parser, fire, google-api-python-client, google-auth, google-cloud-storage, jsonschema, kfp-pipeline-spec, kfp-server-api, kubernetes, protobuf, pydantic, PyYAML, requests-toolbelt, strip-hints, tabulate, typer, typing-extensions, uritemplate\n",
      "Required-by: kubeflow-kale\n"
     ]
    }
   ],
   "source": [
    "# confirm the kfp sdk\n",
    "! pip show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B2CjdRcuRgT"
   },
   "source": [
    "## Import kubeflow pipeline libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_8P4-rCDtnXT"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWPLyw_DuzSl"
   },
   "source": [
    "## Kubeflow pipeline component creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98g9LoIcuaPB"
   },
   "source": [
    " Component 1: mnist_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lGK3hlXdtnXV"
   },
   "outputs": [],
   "source": [
    "# download data step\n",
    "def download_load_preprocess_data(download_link: str, output_data_path: OutputPath(str)):\n",
    "    \n",
    "    import os\n",
    "    import zipfile\n",
    "    import pickle\n",
    "    import wget\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Check and create output data path\n",
    "    if not os.path.exists(output_data_path):\n",
    "        os.makedirs(output_data_path)\n",
    "    \n",
    "    # Step 1: Download Data\n",
    "    wget.download(download_link.format(file='train'), f'{output_data_path}/train_csv.zip')\n",
    "    wget.download(download_link.format(file='test'), f'{output_data_path}/test_csv.zip')\n",
    "    \n",
    "    with zipfile.ZipFile(f\"{output_data_path}/train_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_data_path)\n",
    "        \n",
    "    with zipfile.ZipFile(f\"{output_data_path}/test_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_data_path)\n",
    "\n",
    "    # Step 2: Load Data\n",
    "    train_data_path = os.path.join(output_data_path, 'train.csv')\n",
    "    test_data_path = os.path.join(output_data_path, 'test.csv')\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    ntrain = train_df.shape[0]\n",
    "    all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "    print(\"all_data size is : {}\".format(all_data.shape))\n",
    "\n",
    "    # Step 3: Preprocess Data\n",
    "    all_data_X = all_data.drop('label', axis=1)\n",
    "    all_data_y = all_data.label\n",
    "\n",
    "    all_data_X = all_data_X.values.reshape(-1, 28, 28, 1)\n",
    "    all_data_X = all_data_X / 255.0\n",
    "\n",
    "    X = all_data_X[:ntrain].copy()\n",
    "    y = all_data_y[:ntrain].copy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    with open(f'{output_data_path}/train', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    with open(f'{output_data_path}/test', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9Yhdk3xudcn"
   },
   "source": [
    "Component 2: load the digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oEdsQpH2tnXX"
   },
   "outputs": [],
   "source": [
    "# model and predict\n",
    "def modeling_and_prediction(preprocess_data_path: str, \n",
    "                            model_path: str):\n",
    "    import os\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from tensorflow import keras, optimizers\n",
    "    from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "    from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "    from tensorflow.keras.models import load_model\n",
    "    # Step 1: Modeling\n",
    "    # Load train data\n",
    "    with open(f'{preprocess_data_path}/train', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "        \n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, y_train = train_data\n",
    "    \n",
    "    # Initializing the model\n",
    "    hidden_dim1 = 56\n",
    "    hidden_dim2 = 100\n",
    "    DROPOUT = 0.5\n",
    "    model = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=hidden_dim1, kernel_size=(5,5), padding='Same', activation='relu'),\n",
    "            keras.layers.Dropout(DROPOUT),\n",
    "            keras.layers.Conv2D(filters=hidden_dim2, kernel_size=(3,3), padding='Same', activation='relu'),\n",
    "            keras.layers.Dropout(DROPOUT),\n",
    "            keras.layers.Conv2D(filters=hidden_dim2, kernel_size=(3,3), padding='Same', activation='relu'),\n",
    "            keras.layers.Dropout(DROPOUT),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(10, activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "    model.build(input_shape=(None, 28, 28, 1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizers.Adam(learning_rate=0.001), \n",
    "                  loss=SparseCategoricalCrossentropy(), \n",
    "                  metrics=SparseCategoricalAccuracy(name='accuracy'))\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(np.array(X_train), np.array(y_train), validation_split=0.1, epochs=1, batch_size=64)\n",
    "    \n",
    "    # Load test data\n",
    "    with open(f'{preprocess_data_path}/test', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    \n",
    "    # Separate X_test and y_test\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "    print(\"Test_loss: {}, Test_accuracy: {}\".format(test_loss, test_acc))\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model.save(f'{model_path}/model.h5')\n",
    "\n",
    "    # Step 2: Prediction\n",
    "    # Load the model\n",
    "    model = load_model(f'{model_path}/model.h5')\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    vocab = list(np.unique(y_test))\n",
    "\n",
    "    # Process confusion matrix data\n",
    "    data = [(vocab[target_index], vocab[predicted_index], count) for target_index, target_row in enumerate(cm) for predicted_index, count in enumerate(target_row)]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    df[['target', 'predicted']] = df[['target', 'predicted']].astype(int).astype(str)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"count\", \"type\": \"NUMBER\"}\n",
    "                ],\n",
    "                \"source\": df.to_csv(header=False, index=False),\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Save metadata\n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    conf_m_result = namedtuple('conf_m_result', ['mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return conf_m_result(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_load_preprocess_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create light weight components\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m download_load_preprocess_data_op \u001b[38;5;241m=\u001b[39m comp\u001b[38;5;241m.\u001b[39mcreate_component_from_func(\u001b[43mdownload_load_preprocess_data\u001b[49m, base_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchasechristensen/mnist_process:v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,output_component_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_component.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m modeling_and_prediction_op \u001b[38;5;241m=\u001b[39m comp\u001b[38;5;241m.\u001b[39mcreate_component_from_func(modeling_and_prediction, base_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchasechristensen/mnist_predict:v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,output_component_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict_component.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download_load_preprocess_data' is not defined"
     ]
    }
   ],
   "source": [
    "# create light weight components\n",
    "download_load_preprocess_data_op = comp.create_component_from_func(download_load_preprocess_data, base_image=\"chasechristensen/mnist_process:v1\",output_component_file='data_component.yaml')\n",
    "modeling_and_prediction_op = comp.create_component_from_func(modeling_and_prediction, base_image=\"chasechristensen/mnist_predict:v1\",output_component_file='predict_component.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create real components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple\n",
    "data_comp=comp.load_component_from_file(\"./components/consolidated_steps/download_load_preprocess_data/data_component.yaml\")\n",
    "prediction_comp=comp.load_component_from_file(\"./components/consolidated_steps/modeling_and_prediction/predict_component.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create kubeflow pipeline components from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3bNFpBOuuG-"
   },
   "source": [
    "## Kubeflow pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "LVbUms_ptnXc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "ECZRaIgCtnXd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"digit-recognizer-pipeline\", \n",
    "              description=\"Performs Preprocessing, training and prediction of digits\")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def digit_recognize_pipeline(download_link: str,\n",
    "                             model_path:str\n",
    "                            ):\n",
    "\n",
    "\n",
    "    # Create download container.\n",
    "    data_container = data_comp(download_link)\n",
    "    data_container.set_memory_limit('40G')\n",
    "    data_container.set_memory_request('40G')\n",
    "    # Create predict container\n",
    "    predict_container= prediction_comp(data_container.outputs['output_train_data'],data_container.outputs['output_test_data'],model_path)\n",
    "    predict_container.set_memory_limit('4G')\n",
    "    predict_container.set_memory_request('4G')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link = 'https://github.com/kubeflow/examples/blob/master/digit-recognition-kaggle-competition/data/{file}.csv.zip?raw=true'\n",
    "output_data_path = \"/mnt\"\n",
    "model_path = \"/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "Jq2M3chhtnXd",
    "outputId": "cd75c395-f0d1-415f-8205-9ea23d52fdb5"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid memory string. Should be an integer, or integer followed by one of \"E|Ei|P|Pi|T|Ti|G|Gi|M|Mi|K|Ki\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m arguments \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload_link\u001b[39m\u001b[38;5;124m\"\u001b[39m: download_link,\n\u001b[1;32m      7\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m:model_path}\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compile pipeline to generate compressed YAML definition of the pipeline.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mkfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Submit pipeline directly from pipeline function\u001b[39;00m\n\u001b[1;32m     14\u001b[0m run_result \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_run_from_pipeline_func(pipeline_func, \n\u001b[1;32m     15\u001b[0m                                                   experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name, \n\u001b[1;32m     16\u001b[0m                                                   run_name\u001b[38;5;241m=\u001b[39mrun_name, \n\u001b[1;32m     17\u001b[0m                                                   arguments\u001b[38;5;241m=\u001b[39marguments\n\u001b[1;32m     18\u001b[0m                                                  )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kfp/compiler/compiler.py:1175\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[0;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1174\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mTYPE_CHECK \u001b[38;5;241m=\u001b[39m type_check\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_write_workflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mTYPE_CHECK \u001b[38;5;241m=\u001b[39m type_check_old_value\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kfp/compiler/compiler.py:1227\u001b[0m, in \u001b[0;36mCompiler._create_and_write_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_and_write_workflow\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1219\u001b[0m                                pipeline_func: Callable,\n\u001b[1;32m   1220\u001b[0m                                pipeline_name: Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1223\u001b[0m                                pipeline_conf: dsl\u001b[38;5;241m.\u001b[39mPipelineConf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1224\u001b[0m                                package_path: Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compile the given pipeline function and dump it to specified file\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    format.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m     workflow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpipeline_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpipeline_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_workflow(workflow, package_path)\n\u001b[1;32m   1231\u001b[0m     _validate_workflow(workflow)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kfp/compiler/compiler.py:1005\u001b[0m, in \u001b[0;36mCompiler._create_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         args_list\u001b[38;5;241m.\u001b[39mappend(param)\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dsl\u001b[38;5;241m.\u001b[39mPipeline(pipeline_name) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[43mpipeline_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m pipeline_conf \u001b[38;5;241m=\u001b[39m pipeline_conf \u001b[38;5;129;01mor\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mconf  \u001b[38;5;66;03m# Configuration passed to the compiler is overriding. Unfortunately, it's not trivial to detect whether the dsl_pipeline.conf was ever modified.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_exit_handler(dsl_pipeline)\n",
      "Cell \u001b[0;32mIn[120], line 13\u001b[0m, in \u001b[0;36mdigit_recognize_pipeline\u001b[0;34m(download_link, model_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdigit-recognizer-pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m               description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforms Preprocessing, training and prediction of digits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Create download container.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     data_container \u001b[38;5;241m=\u001b[39m data_comp(download_link)\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mdata_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_memory_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m40g\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     data_container\u001b[38;5;241m.\u001b[39mset_memory_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m40G\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Create predict container\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kfp/dsl/_container_op.py:79\u001b[0m, in \u001b[0;36mdeprecation_warning.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`dsl.ContainerOp.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m` will be removed in future releases. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `dsl.ContainerOp.container.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m     78\u001b[0m         (op_name, container_name), \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kfp/dsl/_container_op.py:1289\u001b[0m, in \u001b[0;36mContainerOp.__init__.<locals>._proxy.<locals>._decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorated\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;66;03m# execute method\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_container\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container:\n\u001b[1;32m   1291\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kfp/dsl/_container_op.py:340\u001b[0m, in \u001b[0;36mContainer.set_memory_limit\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set memory limit (maximum) for this operator.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m  memory(Union[str, PipelineParam]): a string which can be a number or a number followed by one of\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"E\", \"P\", \"T\", \"G\", \"M\", \"K\".\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(memory, _pipeline_param\u001b[38;5;241m.\u001b[39mPipelineParam):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_size_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_spec:\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_spec\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mmemory_limit \u001b[38;5;241m=\u001b[39m _get_resource_number(\n\u001b[1;32m    343\u001b[0m             memory)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/kfp/dsl/_container_op.py:227\u001b[0m, in \u001b[0;36mContainer._validate_size_string\u001b[0;34m(self, size_string)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^[0-9]+(E|Ei|P|Pi|T|Ti|G|Gi|M|Mi|K|Ki)\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0,1}$\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    226\u001b[0m             size_string) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid memory string. Should be an integer, or integer followed \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE|Ei|P|Pi|T|Ti|G|Gi|M|Mi|K|Ki\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid memory string. Should be an integer, or integer followed by one of \"E|Ei|P|Pi|T|Ti|G|Gi|M|Mi|K|Ki\""
     ]
    }
   ],
   "source": [
    "pipeline_func = digit_recognize_pipeline\n",
    "\n",
    "experiment_name = 'digit_recognizer_hardened04'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"download_link\": download_link,\n",
    "             \"model_path\":model_path}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments\n",
    "                                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "kubepipe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "experiment": {
    "id": "6f6c9b81-54e3-414b-974a-6fe8b445a59e",
    "name": "digit_recognize_lightweight"
   },
   "experiment_name": "digit_recognize_lightweight",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "Performs Preprocessing, training and prediction of digits",
   "pipeline_name": "digit-recognizer-kfp",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "demoo-workspace-44q2m",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
